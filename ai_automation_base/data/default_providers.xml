<?xml version="1.0" encoding="utf-8"?>
<odoo>
    <data noupdate="1">
        
        <!-- Default LLM Provider Models -->
        
        <!-- OpenAI Models -->
        <record id="model_openai_gpt4o" model="llm.provider.model">
            <field name="name">GPT-4o</field>
            <field name="model_code">gpt-4o</field>
            <field name="provider_type">openai</field>
            <field name="is_active">True</field>
            <field name="supports_chat">True</field>
            <field name="supports_text_generation">True</field>
            <field name="supports_embeddings">False</field>
            <field name="supports_streaming">True</field>
            <field name="supports_function_calling">True</field>
            <field name="max_tokens">4096</field>
            <field name="context_length">128000</field>
            <field name="input_cost_per_1k_tokens">0.0025</field>
            <field name="output_cost_per_1k_tokens">0.01</field>
            <field name="description">GPT-4o is OpenAI's most advanced model, optimized for speed and cost. It can handle text, images, and audio inputs.</field>
        </record>
        
        <record id="model_openai_gpt4o_mini" model="llm.provider.model">
            <field name="name">GPT-4o Mini</field>
            <field name="model_code">gpt-4o-mini</field>
            <field name="provider_type">openai</field>
            <field name="is_active">True</field>
            <field name="supports_chat">True</field>
            <field name="supports_text_generation">True</field>
            <field name="supports_embeddings">False</field>
            <field name="supports_streaming">True</field>
            <field name="supports_function_calling">True</field>
            <field name="max_tokens">4096</field>
            <field name="context_length">128000</field>
            <field name="input_cost_per_1k_tokens">0.00015</field>
            <field name="output_cost_per_1k_tokens">0.0006</field>
            <field name="description">GPT-4o Mini is a faster and more cost-effective version of GPT-4o, optimized for high-volume tasks.</field>
        </record>
        
        <record id="model_openai_text_embedding_ada" model="llm.provider.model">
            <field name="name">Text Embedding Ada</field>
            <field name="model_code">text-embedding-ada-002</field>
            <field name="provider_type">openai</field>
            <field name="is_active">True</field>
            <field name="supports_chat">False</field>
            <field name="supports_text_generation">False</field>
            <field name="supports_embeddings">True</field>
            <field name="supports_streaming">False</field>
            <field name="supports_function_calling">False</field>
            <field name="max_tokens">8191</field>
            <field name="context_length">8191</field>
            <field name="input_cost_per_1k_tokens">0.0001</field>
            <field name="output_cost_per_1k_tokens">0.0</field>
            <field name="description">Text embedding model for converting text into numerical representations.</field>
        </record>
        
        <!-- Google Gemini Models -->
        <record id="model_gemini_2_5_pro" model="llm.provider.model">
            <field name="name">Gemini 2.5 Pro</field>
            <field name="model_code">gemini-2.5-pro</field>
            <field name="provider_type">gemini</field>
            <field name="is_active">True</field>
            <field name="supports_chat">True</field>
            <field name="supports_text_generation">True</field>
            <field name="supports_embeddings">False</field>
            <field name="supports_streaming">True</field>
            <field name="supports_function_calling">True</field>
            <field name="max_tokens">8192</field>
            <field name="context_length">1000000</field>
            <field name="input_cost_per_1k_tokens">0.0035</field>
            <field name="output_cost_per_1k_tokens">0.0105</field>
            <field name="description">Gemini 2.5 Pro is Google's most advanced model with extensive context window and multimodal capabilities.</field>
        </record>
        
        <record id="model_gemini_2_5_flash" model="llm.provider.model">
            <field name="name">Gemini 2.5 Flash</field>
            <field name="model_code">gemini-2.5-flash</field>
            <field name="provider_type">gemini</field>
            <field name="is_active">True</field>
            <field name="supports_chat">True</field>
            <field name="supports_text_generation">True</field>
            <field name="supports_embeddings">False</field>
            <field name="supports_streaming">True</field>
            <field name="supports_function_calling">True</field>
            <field name="max_tokens">8192</field>
            <field name="context_length">1000000</field>
            <field name="input_cost_per_1k_tokens">0.000075</field>
            <field name="output_cost_per_1k_tokens">0.0003</field>
            <field name="description">Gemini 2.5 Flash is optimized for speed and cost-effectiveness while maintaining high quality.</field>
        </record>
        
        <record id="model_gemini_embedding" model="llm.provider.model">
            <field name="name">Text Embedding</field>
            <field name="model_code">embedding-001</field>
            <field name="provider_type">gemini</field>
            <field name="is_active">True</field>
            <field name="supports_chat">False</field>
            <field name="supports_text_generation">False</field>
            <field name="supports_embeddings">True</field>
            <field name="supports_streaming">False</field>
            <field name="supports_function_calling">False</field>
            <field name="max_tokens">2048</field>
            <field name="context_length">2048</field>
            <field name="input_cost_per_1k_tokens">0.0001</field>
            <field name="output_cost_per_1k_tokens">0.0</field>
            <field name="description">Google's text embedding model for converting text into numerical representations.</field>
        </record>
        
        <!-- Anthropic Claude Models -->
        <record id="model_claude_3_5_sonnet" model="llm.provider.model">
            <field name="name">Claude 3.5 Sonnet</field>
            <field name="model_code">claude-3-5-sonnet-20241022</field>
            <field name="provider_type">claude</field>
            <field name="is_active">True</field>
            <field name="supports_chat">True</field>
            <field name="supports_text_generation">True</field>
            <field name="supports_embeddings">False</field>
            <field name="supports_streaming">True</field>
            <field name="supports_function_calling">True</field>
            <field name="max_tokens">4096</field>
            <field name="context_length">200000</field>
            <field name="input_cost_per_1k_tokens">0.003</field>
            <field name="output_cost_per_1k_tokens">0.015</field>
            <field name="description">Claude 3.5 Sonnet is Anthropic's most capable model, excelling at complex reasoning and analysis.</field>
        </record>
        
        <record id="model_claude_3_5_haiku" model="llm.provider.model">
            <field name="name">Claude 3.5 Haiku</field>
            <field name="model_code">claude-3-5-haiku-20241022</field>
            <field name="provider_type">claude</field>
            <field name="is_active">True</field>
            <field name="supports_chat">True</field>
            <field name="supports_text_generation">True</field>
            <field name="supports_embeddings">False</field>
            <field name="supports_streaming">True</field>
            <field name="supports_function_calling">True</field>
            <field name="max_tokens">4096</field>
            <field name="context_length">200000</field>
            <field name="input_cost_per_1k_tokens">0.00025</field>
            <field name="output_cost_per_1k_tokens">0.00125</field>
            <field name="description">Claude 3.5 Haiku is optimized for speed and cost, perfect for high-volume applications.</field>
        </record>
        
        <record id="model_claude_3_opus" model="llm.provider.model">
            <field name="name">Claude 3 Opus</field>
            <field name="model_code">claude-3-opus-20240229</field>
            <field name="provider_type">claude</field>
            <field name="is_active">True</field>
            <field name="supports_chat">True</field>
            <field name="supports_text_generation">True</field>
            <field name="supports_embeddings">False</field>
            <field name="supports_streaming">True</field>
            <field name="supports_function_calling">True</field>
            <field name="max_tokens">4096</field>
            <field name="context_length">200000</field>
            <field name="input_cost_per_1k_tokens">0.015</field>
            <field name="output_cost_per_1k_tokens">0.075</field>
            <field name="description">Claude 3 Opus is Anthropic's most powerful model for complex tasks requiring deep reasoning.</field>
        </record>
        
    </data>
</odoo> 